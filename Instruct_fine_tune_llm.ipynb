{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-rhmFwQ7yT2",
        "outputId": "a44fa159-0c64-484f-c1ed-19e10a7e20cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "! pip install transformers --quiet\n",
        "! pip install datasets --quiet\n",
        "! pip install trl --quiet #Transformer Reinforcement Learning\n",
        "#! pip install peft\n",
        "! pip install git+https://github.com/huggingface/peft.git --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "#dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\n",
        "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
        "#dataset[0]"
      ],
      "metadata": {
        "id": "SQ3WHUm6D7VC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "open_qa = dataset.filter(lambda example: example['context'] == \"\")\n",
        "print(len(dataset), len(open_qa))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_kk9rEmECjB",
        "outputId": "81d7c710-a154-4805-c29e-ca00daa95cca"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15011 10544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def formatting_prompts_func(example):\n",
        "    output_texts = []\n",
        "    for i in range(len(example['instruction'])):\n",
        "        text = f\"### Question: {example['instruction'][i]}\\n ### Answer: {example['response'][i]}\"\n",
        "        output_texts.append(text)\n",
        "    return output_texts"
      ],
      "metadata": {
        "id": "WJXLLqkVA0SN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel, PeftConfig, LoraConfig"
      ],
      "metadata": {
        "id": "d4vfahkFH769"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    \"EleutherAI/gpt-neo-125m\",\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    peft_config=peft_config\n",
        ")'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "eVylToI4JsNW",
        "outputId": "8330783e-ea96-4cfd-8d46-ccfa6f47f552"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\npeft_config = LoraConfig(\\n    r=16,\\n    lora_alpha=32,\\n    lora_dropout=0.05,\\n    bias=\"none\",\\n    task_type=\"CAUSAL_LM\",\\n)\\n\\ntrainer = SFTTrainer(\\n    \"EleutherAI/gpt-neo-125m\",\\n    train_dataset=dataset,\\n    dataset_text_field=\"text\",\\n    peft_config=peft_config\\n)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import gc\n",
        "import torch\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()'\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "d6Yp28CzQ5DL",
        "outputId": "d0915846-3d94-459c-8e59-3598e1510b99"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nimport gc\\nimport torch\\ngc.collect()\\ntorch.cuda.empty_cache()'\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
        "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
        "\n",
        "base_model = \"distilgpt2\"\n",
        "#base_model = \"facebook/opt-350m\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model)\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    #tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    #model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZWukL27-3Yg",
        "outputId": "52193017-537d-483b-d6ce-32d72dd35806"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using pad_token, but it is not set yet.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = tokenizer('What is meaning of life', return_tensors='pt')\n",
        "output_ftm = model.generate(**input_text)\n",
        "print(tokenizer.decode(output_ftm[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ph-NjD_PbEk9",
        "outputId": "1c314964-d3d3-4cfb-a85c-ff4f761f5d6e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is meaning of life?\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = base_model.split(\"/\")[-1]\n",
        "training_args = TrainingArguments(\n",
        "    f\"{model_name}-finetuned-qna\",\n",
        "    evaluation_strategy = \"no\",\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    num_train_epochs = 1,\n",
        "    push_to_hub=False,\n",
        "    do_eval=False,\n",
        ")\n",
        "\n",
        "response_template = \" ### Answer:\"\n",
        "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model,\n",
        "    train_dataset=open_qa,\n",
        "    args=training_args,\n",
        "    formatting_func=formatting_prompts_func,\n",
        "    #peft_config=peft_config,\n",
        "    data_collator=collator,\n",
        "    max_seq_length=256,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "xdV7v7QHa9EO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = tokenizer('### Question: What is meaning of life', return_tensors='pt').to(\"cuda\")\n",
        "# input_text = tokenizer('what is life ?', return_tensors='pt').to(\"cuda\")\n",
        "output_ftm = model.generate(**input_text, max_new_tokens=20)\n",
        "print(tokenizer.decode(output_ftm[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9kOI0jjWn4W",
        "outputId": "df18897b-758d-4929-ee21-1d956e79d09f"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Question: What is meaning of life?\n",
            "\n",
            "Life is a process of living in a world that is not only a process of living\n"
          ]
        }
      ]
    }
  ]
}